task_attributes:
  data_root_dir: data
  batch_size: 512
  vocab_file: visdial/visdial_vocabulary.txt
  max_seq_len: 20
  max_history_len: 60
  embedding_name: glove.6B.300d
  image_depth_first: false
  image_fast_reader: true
  image_feat_test:
  - rcnn_10_100/vqa/val2014
  image_feat_train:
  - rcnn_10_100/vqa/train2014
  image_feat_val:
  - rcnn_10_100/vqa/val2014
  image_max_loc: 100
  imdb_file_test:
  - visdial/visdial_val_imdb.json
  imdb_file_train:
  - visdial/visdial_train_imdb.json
  imdb_file_val:
  - visdial/visdial_val_imdb.json
  num_workers: 5
  enforce_slow_reader: false
exp_name: baseline
loss: logits_bce
model_attributes:
  visdial_top_down_bottom_up:
    classifier:
      type: logit
      params:
        img_hidden_dim: 5000
        text_hidden_dim: 300
    image_embeddings:
    - modal_combine:
        type: gated_element_multiply
        params:
          dropout: 0
          hidden_dim: 5000
      normalization: softmax
      transform:
        type: linear
        params:
          out_dim: 1
    image_feature_dim: 2048
    image_feature_encodings:
    - type: default
      params: {}
    modal_combine:
      type: gated_element_multiply
      params:
        dropout: 0
        hidden_dim: 5000
    question_embeddings:
    - type: attention
      params:
        hidden_dim: 1024
        num_layers: 1
        conv1_out: 512
        conv2_out: 2
        dropout: 0
        embedding_dim: 300
        embedding_init_file: large_vqa2.0_glove.6B.300d.txt.npy
        kernel_size: 1
        padding: 0
optimizer_attributes:
  type: Adamax
  params:
    eps: 1.0e-08
    lr: 0.01
    weight_decay: 0
run: train+predict
training_parameters:
  clip_norm_mode: all
  clip_gradients: true
  lr_ratio: 0.1
  lr_steps:
  - 15000
  - 18000
  - 20000
  - 21000
  max_grad_l2_norm: 0.25
  max_iterations: 22000
  log_interval: 100
  snapshot_interval: 1000
  wu_factor: 0.2
  wu_iters: 1000
