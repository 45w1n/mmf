task_attributes:
  vqa:
    datasets: all
    dataset_size_proportional_sampling: true
    dataset_attributes:
      vqa2:
        data_root_dir: ../data/
        image_depth_first: false
        image_fast_reader: false
        image_feat_test:
        - /checkpoint/asg/old_checkpoint02/data/detectron_fix_100/fc6/vqa/test2015
        image_feat_train:
        - /checkpoint/asg/old_checkpoint02/data/detectron_fix_100/fc6/vqa/train2014
        - /checkpoint/asg/old_checkpoint02/data/detectron_fix_100/fc6/vqa/val2014
        image_feat_val:
        - /checkpoint/asg/old_checkpoint02/data/detectron_fix_100/fc6/vqa/val2014
        image_max_loc: 100
        imdb_file_test:
        - imdb/imdb_test2015.npy
        imdb_file_train:
        - imdb/imdb_train2014.npy
        - imdb/imdb_val2014.npy
        imdb_file_val:
        - imdb/imdb_minival2014.npy
        question_max_len: 14
        vocab_answer_file: answers_vqa.txt
        vocab_question_file: vocabulary_vqa.txt
        enforce_slow_reader: true
        metrics: vqa_accuracy
        context_max_len: 50
        loss: logit_bce
        copy_included: false
        copy_type: soft
        # loss:
        #   type: multi
        #   params:
        #     - type: nll
        #       weight: 0.7
        #     - type: attention_supervision
        #       weight: 0.3
        loss: logit_bce
        return_info: true
        use_ocr: true
exp_name: baseline
model_attributes:
  top_down_bottom_up:
    model_data_dir: ../data
    classifier:
      type: logit
      params:
        img_hidden_dim: 2500
        text_hidden_dim: 300
    image_feature_embeddings:
    - modal_combine:
        type: non_linear_element_multiply
        params:
          dropout: 0.2
          hidden_dim: 2500
      normalization: softmax
      transform:
        type: linear
        params:
          out_dim: 1
    image_feature_dim: 2048
    image_feature_encodings:
    - type: finetune_faster_rcnn_fpn_fc7
      params:
        bias_file: detec/detectron/fc6/fc7_b.pkl
        weights_file: detec/detectron/fc6/fc7_w.pkl
    image_text_modal_combine:
      type: non_linear_element_multiply
      params:
        dropout: 0.2
        hidden_dim: 2500
    text_embeddings:
    - type: attention
      params:
        hidden_dim: 1024
        num_layers: 1
        conv1_out: 512
        conv2_out: 2
        dropout: 0.2
        embedding_dim: 300
        # embedding_init_file: vqa2.0_glove.6B.300d.txt.npy
        kernel_size: 1
        padding: 0
  vizwiz_top_down_bottom_up_soft_copy:
    model_data_dir: ../data
    num_context_features: 1
    context_feature_dim: 300
    image_feature_dim: 2048
    context_max_len: 50
    classifier:
      type: logit
      params:
        img_hidden_dim: 2500
        text_hidden_dim: 300
    context_classifier:
      type: logit
      params:
        img_hidden_dim: 2500
        text_hidden_dim: 300
    image_feature_embeddings:
    - modal_combine:
        type: non_linear_element_multiply
        params:
          dropout: 0.2
          hidden_dim: 2500
      normalization: softmax
      transform:
        type: linear
        params:
          out_dim: 1
    context_feature_embeddings:
    - modal_combine:
        type: non_linear_element_multiply
        params:
          dropout: 0.2
          hidden_dim: 2500
      normalization: sigmoid
      transform:
        type: linear
        params:
          out_dim: 1
    image_feature_encodings:
    - type: finetune_faster_rcnn_fpn_fc7
      params:
        bias_file: detec/detectron/fc6/fc7_b.pkl
        weights_file: detec/detectron/fc6/fc7_w.pkl
    context_feature_encodings:
    - type: default
      params: {}
    image_text_modal_combine:
      type: non_linear_element_multiply
      params:
        dropout: 0.2
        hidden_dim: 2500
    context_text_modal_combine:
      type: non_linear_element_multiply
      params:
        dropout: 0.2
        hidden_dim: 2500
    text_embeddings:
    - type: attention
      params:
        hidden_dim: 1024
        num_layers: 1
        conv1_out: 512
        conv2_out: 2
        dropout: 0.2
        embedding_dim: 300
        kernel_size: 1
        padding: 0
    context_embeddings:
    - type: torch
      params:
        embedding_dim: 300
optimizer_attributes:
  type: Adamax
  params:
    eps: 1.0e-08
    lr: 0.01
    weight_decay: 0
training_parameters:
  clip_norm_mode: all
  clip_gradients: true
  lr_ratio: 0.1
  lr_scheduler: true
  lr_steps:
  - 15000
  - 18000
  - 20000
  - 21000
  max_grad_l2_norm: 0.25
  max_iterations: 22000
  log_interval: 1000
  snapshot_interval: 6000
  wu_factor: 0.2
  wu_iters: 1000
  patience: 4000
  batch_size: 512
  num_workers: 7
  task_size_proportional_sampling: true
  run_type: train+predict
  monitored_metric: vqa2_vqa_accuracy
  metric_minimize: false
  text_vocab:
    type: intersected
    embedding_name: glove.6B.300d
    vocab_file: ../data/vocabulary_100k.txt
  context_vocab:
    type: model
    name: fasttext
    model_file: .vector_cache/wiki.en.bin
